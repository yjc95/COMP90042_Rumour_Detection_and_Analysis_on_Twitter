{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result = pd.read_csv('tweet_covid_result_bert.csv')\n",
    "covid = pd.read_csv('./project-data/tweet_covid_bert.tsv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "          Id  Predicted\n0          0          0\n1          1          0\n2          2          0\n3          3          0\n4          4          0\n...      ...        ...\n17453  17453          0\n17454  17454          0\n17455  17455          0\n17456  17456          0\n17457  17457          0\n\n[17458 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>17453</th>\n      <td>17453</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17454</th>\n      <td>17454</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17455</th>\n      <td>17455</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17456</th>\n      <td>17456</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17457</th>\n      <td>17457</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>17458 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                sentence\n0      according new york times warner bros wanted de...\n1      hurricane hanna made landfall texas storm hitt...\n2      monkeys loose india stolen coronavirus blood s...\n3      let play blind work fighting islamic religion ...\n4      trump felt comfortable comfortable said one su...\n...                                                  ...\n17453  wonder many lives could saved trump focused fi...\n17454  front page th march first line reads britain p...\n17455  trump completed racism trifecta three minute s...\n17456  photographs today division proceedings pandemi...\n17457  gone bill de blasio says nyc facing billion de...\n\n[17458 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>according new york times warner bros wanted de...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>hurricane hanna made landfall texas storm hitt...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>monkeys loose india stolen coronavirus blood s...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>let play blind work fighting islamic religion ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>trump felt comfortable comfortable said one su...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>17453</th>\n      <td>wonder many lives could saved trump focused fi...</td>\n    </tr>\n    <tr>\n      <th>17454</th>\n      <td>front page th march first line reads britain p...</td>\n    </tr>\n    <tr>\n      <th>17455</th>\n      <td>trump completed racism trifecta three minute s...</td>\n    </tr>\n    <tr>\n      <th>17456</th>\n      <td>photographs today division proceedings pandemi...</td>\n    </tr>\n    <tr>\n      <th>17457</th>\n      <td>gone bill de blasio says nyc facing billion de...</td>\n    </tr>\n  </tbody>\n</table>\n<p>17458 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "new_data = pd.DataFrame()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "new_data['sentence'] = covid['sentence'].copy()\n",
    "new_data['Predicted'] = result['Predicted'].copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "new_data_rumor = new_data[new_data['Predicted'] == 1]\n",
    "new_data_nonrumor = new_data[new_data['Predicted'] == 0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "# pd.to_csv(new_data,'./tweet_covid_bert.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "def get_LDA(new_data = new_data):\n",
    "    def lemmatize_stemming(text):\n",
    "        return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "    def preprocess(text):\n",
    "        result = []\n",
    "        for token in gensim.utils.simple_preprocess(text):\n",
    "            if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "                result.append(lemmatize_stemming(token))\n",
    "        return result\n",
    "    import gensim\n",
    "    from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "    import numpy as np\n",
    "    np.random.seed(2018)\n",
    "    import nltk\n",
    "    nltk.download('wordnet')\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    processed_data = new_data['sentence'].map(preprocess)\n",
    "    dictionary = gensim.corpora.Dictionary(processed_data)\n",
    "    count = 0\n",
    "    # for k, v in dictionary.iteritems():\n",
    "    #     print(k, v)\n",
    "    #     count += 1\n",
    "    #     if count > 10:\n",
    "    #         break\n",
    "    dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_data]\n",
    "    from gensim import corpora, models\n",
    "\n",
    "    tfidf = models.TfidfModel(bow_corpus)\n",
    "    corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "    print('###################LDA BOW######################')\n",
    "    lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=None)\n",
    "    for idx, topic in lda_model.print_topics(-1):\n",
    "        print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "    print('###################LDA TFIDF######################')\n",
    "    lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "    for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "        print('Topic: {} Word: {}'.format(idx, topic))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yujia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################LDA BOW######################\n",
      "Topic: 0 \n",
      "Words: 0.072*\"coronavirus\" + 0.023*\"covid\" + 0.023*\"trump\" + 0.020*\"peopl\" + 0.010*\"think\" + 0.010*\"test\" + 0.010*\"elect\" + 0.010*\"like\" + 0.010*\"corona\" + 0.009*\"american\"\n",
      "Topic: 1 \n",
      "Words: 0.050*\"covid\" + 0.039*\"trump\" + 0.038*\"coronavirus\" + 0.017*\"american\" + 0.015*\"death\" + 0.015*\"peopl\" + 0.014*\"like\" + 0.012*\"know\" + 0.009*\"die\" + 0.009*\"say\"\n",
      "Topic: 2 \n",
      "Words: 0.047*\"covid\" + 0.036*\"coronavirus\" + 0.023*\"school\" + 0.020*\"trump\" + 0.019*\"want\" + 0.018*\"like\" + 0.018*\"peopl\" + 0.011*\"children\" + 0.010*\"death\" + 0.009*\"think\"\n",
      "Topic: 3 \n",
      "Words: 0.041*\"covid\" + 0.041*\"coronavirus\" + 0.031*\"death\" + 0.030*\"american\" + 0.023*\"trump\" + 0.017*\"peopl\" + 0.016*\"rat\" + 0.014*\"die\" + 0.013*\"kill\" + 0.012*\"case\"\n",
      "Topic: 4 \n",
      "Words: 0.128*\"coronavirus\" + 0.057*\"elect\" + 0.034*\"trump\" + 0.024*\"general\" + 0.021*\"worri\" + 0.016*\"like\" + 0.015*\"covid\" + 0.015*\"peopl\" + 0.011*\"kill\" + 0.010*\"think\"\n",
      "Topic: 5 \n",
      "Words: 0.066*\"covid\" + 0.032*\"trump\" + 0.030*\"coronavirus\" + 0.023*\"death\" + 0.015*\"peopl\" + 0.013*\"like\" + 0.012*\"american\" + 0.010*\"china\" + 0.009*\"virus\" + 0.009*\"case\"\n",
      "Topic: 6 \n",
      "Words: 0.076*\"hoax\" + 0.029*\"coronavirus\" + 0.029*\"covid\" + 0.025*\"believ\" + 0.024*\"trump\" + 0.017*\"peopl\" + 0.016*\"like\" + 0.013*\"american\" + 0.013*\"lie\" + 0.012*\"think\"\n",
      "Topic: 7 \n",
      "Words: 0.025*\"peopl\" + 0.022*\"coronavirus\" + 0.022*\"trump\" + 0.019*\"test\" + 0.015*\"china\" + 0.014*\"like\" + 0.013*\"covid\" + 0.011*\"pandem\" + 0.011*\"govern\" + 0.010*\"wuhan\"\n",
      "Topic: 8 \n",
      "Words: 0.075*\"coronavirus\" + 0.031*\"trump\" + 0.022*\"china\" + 0.021*\"peopl\" + 0.017*\"covid\" + 0.015*\"wuhan\" + 0.013*\"virus\" + 0.012*\"blame\" + 0.011*\"know\" + 0.010*\"presid\"\n",
      "Topic: 9 \n",
      "Words: 0.056*\"coronavirus\" + 0.042*\"virus\" + 0.035*\"wuhan\" + 0.034*\"covid\" + 0.023*\"peopl\" + 0.022*\"chines\" + 0.021*\"china\" + 0.015*\"like\" + 0.014*\"call\" + 0.012*\"trump\"\n",
      "###################LDA TFIDF######################\n",
      "Topic: 0 Word: 0.021*\"coronavirus\" + 0.019*\"covid\" + 0.014*\"trump\" + 0.011*\"wuhan\" + 0.009*\"virus\" + 0.009*\"china\" + 0.008*\"death\" + 0.008*\"peopl\" + 0.008*\"like\" + 0.007*\"test\"\n",
      "Topic: 1 Word: 0.017*\"coronavirus\" + 0.009*\"china\" + 0.009*\"sell\" + 0.008*\"trump\" + 0.008*\"declar\" + 0.008*\"like\" + 0.007*\"economi\" + 0.007*\"peopl\" + 0.007*\"kill\" + 0.007*\"threat\"\n",
      "Topic: 2 Word: 0.010*\"trump\" + 0.009*\"say\" + 0.009*\"day\" + 0.008*\"covid\" + 0.008*\"china\" + 0.008*\"retweet\" + 0.008*\"like\" + 0.008*\"think\" + 0.007*\"peopl\" + 0.007*\"congress\"\n",
      "Topic: 3 Word: 0.009*\"coronavirus\" + 0.008*\"covid\" + 0.008*\"media\" + 0.007*\"like\" + 0.007*\"stori\" + 0.007*\"nurs\" + 0.007*\"busi\" + 0.007*\"great\" + 0.007*\"chines\" + 0.006*\"interest\"\n",
      "Topic: 4 Word: 0.010*\"fauci\" + 0.009*\"prais\" + 0.008*\"money\" + 0.008*\"say\" + 0.008*\"see\" + 0.008*\"rest\" + 0.008*\"actual\" + 0.007*\"blame\" + 0.007*\"news\" + 0.007*\"charg\"\n",
      "Topic: 5 Word: 0.014*\"coronavirus\" + 0.010*\"covid\" + 0.010*\"trump\" + 0.008*\"china\" + 0.008*\"say\" + 0.007*\"bodi\" + 0.007*\"death\" + 0.006*\"politician\" + 0.006*\"biden\" + 0.006*\"right\"\n",
      "Topic: 6 Word: 0.011*\"trump\" + 0.010*\"coronavirus\" + 0.009*\"give\" + 0.008*\"peopl\" + 0.007*\"republican\" + 0.007*\"putin\" + 0.007*\"american\" + 0.007*\"covid\" + 0.007*\"want\" + 0.006*\"money\"\n",
      "Topic: 7 Word: 0.012*\"trump\" + 0.010*\"anti\" + 0.010*\"post\" + 0.009*\"presid\" + 0.009*\"million\" + 0.008*\"countri\" + 0.008*\"pandem\" + 0.008*\"video\" + 0.008*\"flight\" + 0.007*\"claim\"\n",
      "Topic: 8 Word: 0.010*\"articl\" + 0.010*\"ventil\" + 0.009*\"quot\" + 0.009*\"know\" + 0.009*\"coronavirus\" + 0.009*\"say\" + 0.008*\"trump\" + 0.008*\"covid\" + 0.008*\"go\" + 0.007*\"like\"\n",
      "Topic: 9 Word: 0.010*\"coronavirus\" + 0.009*\"video\" + 0.009*\"govern\" + 0.008*\"claim\" + 0.008*\"presid\" + 0.008*\"trump\" + 0.007*\"tweet\" + 0.007*\"covid\" + 0.007*\"human\" + 0.006*\"watch\"\n"
     ]
    }
   ],
   "source": [
    "get_LDA(new_data_rumor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yujia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################LDA BOW######################\n",
      "Topic: 0 \n",
      "Words: 0.020*\"test\" + 0.016*\"peopl\" + 0.016*\"trump\" + 0.015*\"death\" + 0.008*\"mask\" + 0.008*\"like\" + 0.007*\"know\" + 0.006*\"virus\" + 0.006*\"american\" + 0.006*\"presid\"\n",
      "Topic: 1 \n",
      "Words: 0.017*\"peopl\" + 0.017*\"case\" + 0.013*\"trump\" + 0.011*\"death\" + 0.007*\"test\" + 0.007*\"need\" + 0.007*\"know\" + 0.006*\"like\" + 0.006*\"state\" + 0.006*\"countri\"\n",
      "Topic: 2 \n",
      "Words: 0.012*\"trump\" + 0.011*\"peopl\" + 0.010*\"death\" + 0.009*\"pandem\" + 0.009*\"american\" + 0.009*\"test\" + 0.007*\"know\" + 0.006*\"care\" + 0.006*\"countri\" + 0.005*\"home\"\n",
      "Topic: 3 \n",
      "Words: 0.017*\"trump\" + 0.015*\"death\" + 0.012*\"peopl\" + 0.011*\"american\" + 0.008*\"case\" + 0.007*\"like\" + 0.006*\"die\" + 0.006*\"dead\" + 0.006*\"number\" + 0.006*\"pandem\"\n",
      "Topic: 4 \n",
      "Words: 0.021*\"trump\" + 0.014*\"death\" + 0.014*\"american\" + 0.012*\"peopl\" + 0.009*\"state\" + 0.009*\"case\" + 0.008*\"presid\" + 0.008*\"countri\" + 0.008*\"virus\" + 0.006*\"know\"\n",
      "Topic: 5 \n",
      "Words: 0.021*\"trump\" + 0.013*\"peopl\" + 0.012*\"death\" + 0.008*\"pandem\" + 0.008*\"case\" + 0.008*\"american\" + 0.007*\"world\" + 0.006*\"respons\" + 0.006*\"like\" + 0.006*\"go\"\n",
      "Topic: 6 \n",
      "Words: 0.015*\"peopl\" + 0.011*\"china\" + 0.010*\"death\" + 0.009*\"virus\" + 0.009*\"american\" + 0.008*\"trump\" + 0.007*\"like\" + 0.006*\"spread\" + 0.006*\"time\" + 0.006*\"case\"\n",
      "Topic: 7 \n",
      "Words: 0.022*\"trump\" + 0.013*\"peopl\" + 0.012*\"test\" + 0.011*\"death\" + 0.010*\"say\" + 0.010*\"american\" + 0.008*\"virus\" + 0.008*\"pandem\" + 0.008*\"know\" + 0.007*\"die\"\n",
      "Topic: 8 \n",
      "Words: 0.027*\"trump\" + 0.014*\"peopl\" + 0.012*\"death\" + 0.008*\"test\" + 0.008*\"die\" + 0.008*\"case\" + 0.007*\"like\" + 0.007*\"virus\" + 0.006*\"think\" + 0.006*\"pandem\"\n",
      "Topic: 9 \n",
      "Words: 0.018*\"trump\" + 0.014*\"peopl\" + 0.013*\"american\" + 0.012*\"case\" + 0.008*\"death\" + 0.008*\"like\" + 0.006*\"die\" + 0.006*\"time\" + 0.006*\"say\" + 0.006*\"test\"\n",
      "###################LDA TFIDF######################\n",
      "Topic: 0 Word: 0.005*\"test\" + 0.005*\"trump\" + 0.004*\"death\" + 0.004*\"case\" + 0.004*\"peopl\" + 0.003*\"state\" + 0.003*\"school\" + 0.003*\"posit\" + 0.002*\"pandem\" + 0.002*\"say\"\n",
      "Topic: 1 Word: 0.004*\"china\" + 0.004*\"death\" + 0.004*\"trump\" + 0.004*\"peopl\" + 0.003*\"pandem\" + 0.003*\"case\" + 0.003*\"test\" + 0.003*\"world\" + 0.003*\"valid\" + 0.003*\"sentenc\"\n",
      "Topic: 2 Word: 0.004*\"case\" + 0.003*\"test\" + 0.003*\"peopl\" + 0.003*\"china\" + 0.003*\"trump\" + 0.003*\"say\" + 0.003*\"report\" + 0.002*\"death\" + 0.002*\"home\" + 0.002*\"world\"\n",
      "Topic: 3 Word: 0.005*\"trump\" + 0.004*\"test\" + 0.003*\"peopl\" + 0.003*\"death\" + 0.003*\"case\" + 0.003*\"state\" + 0.003*\"say\" + 0.003*\"need\" + 0.003*\"virus\" + 0.002*\"patient\"\n",
      "Topic: 4 Word: 0.004*\"trump\" + 0.004*\"china\" + 0.003*\"test\" + 0.003*\"death\" + 0.003*\"peopl\" + 0.003*\"case\" + 0.003*\"virus\" + 0.003*\"mask\" + 0.003*\"american\" + 0.002*\"say\"\n",
      "Topic: 5 Word: 0.007*\"death\" + 0.005*\"trump\" + 0.005*\"test\" + 0.004*\"peopl\" + 0.003*\"case\" + 0.003*\"die\" + 0.003*\"number\" + 0.003*\"virus\" + 0.003*\"say\" + 0.003*\"like\"\n",
      "Topic: 6 Word: 0.006*\"trump\" + 0.004*\"peopl\" + 0.004*\"test\" + 0.004*\"death\" + 0.003*\"mask\" + 0.003*\"case\" + 0.003*\"american\" + 0.003*\"say\" + 0.003*\"china\" + 0.003*\"like\"\n",
      "Topic: 7 Word: 0.007*\"test\" + 0.006*\"case\" + 0.004*\"trump\" + 0.004*\"posit\" + 0.003*\"report\" + 0.003*\"death\" + 0.003*\"peopl\" + 0.003*\"break\" + 0.003*\"state\" + 0.003*\"record\"\n",
      "Topic: 8 Word: 0.061*\"valid\" + 0.057*\"sentenc\" + 0.003*\"trump\" + 0.003*\"test\" + 0.003*\"death\" + 0.003*\"peopl\" + 0.003*\"case\" + 0.002*\"say\" + 0.002*\"mask\" + 0.002*\"pandem\"\n",
      "Topic: 9 Word: 0.008*\"trump\" + 0.004*\"american\" + 0.004*\"death\" + 0.004*\"peopl\" + 0.004*\"case\" + 0.004*\"test\" + 0.003*\"state\" + 0.003*\"mask\" + 0.003*\"pandem\" + 0.003*\"presid\"\n"
     ]
    }
   ],
   "source": [
    "get_LDA(new_data_nonrumor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}