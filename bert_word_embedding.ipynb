{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f134bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "58de0496",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data, x_dev_data, x_test_data = [], [], []\n",
    "y_train_data, y_dev_data = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "62cf99be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a060d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str):\n",
    "    text = text.lower()\n",
    "    text = remove_emoji(text)\n",
    "\n",
    "    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text)\n",
    "    text = re.sub(r'@(\\w+)?', '', text)\n",
    "    text = re.sub(r'#(\\w+)?', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "#     word_list = [w for w in text.split() if w not in stop_words]\n",
    "#     text_clean = ''\n",
    "#     for w in word_list:\n",
    "#         text_clean += (w + ' ')\n",
    "#     if text_clean != '':\n",
    "#         return text_clean\n",
    "#     return ''\n",
    "    word_list = [w for w in text.split()]\n",
    "    text_clean = ''\n",
    "    for w in word_list:\n",
    "        text_clean += (w + ' ')\n",
    "    return text_clean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "21584375",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./project-data/tweet-train-final.txt', 'r', encoding='utf-8') as f:\n",
    "    tweet_all = f.readlines()\n",
    "    for event in tweet_all:\n",
    "        # print(type(tweets), tweets)\n",
    "        tweets = json.loads(event)\n",
    "        text_event = ''\n",
    "        for k, v in tweets.items():\n",
    "            if 'data' in v:\n",
    "                text = v['data'][0]['text']\n",
    "                text = clean_text(text)\n",
    "                text_event += text\n",
    "        x_train_data.append(text_event)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d5e34d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./project-data/train.label.txt', 'r', encoding='utf-8') as f:\n",
    "    label_all = f.readlines()\n",
    "    # print(type(label_all[1][:-1]), label_all[1][:-1])\n",
    "    for label in label_all:\n",
    "        if label[:-1] == 'rumour':\n",
    "            y_train_data.append(1)\n",
    "        else:\n",
    "            y_train_data.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b0c7b623",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./project-data/tweet-dev-final.txt', 'r', encoding='utf-8') as f:\n",
    "    tweet_all = f.readlines()\n",
    "    for event in tweet_all:\n",
    "        tweets = json.loads(event)\n",
    "        text_event = ''\n",
    "        for k, v in tweets.items():\n",
    "            if 'data' in v:\n",
    "                text = v['data'][0]['text']\n",
    "                text = clean_text(text)\n",
    "                text_event += text\n",
    "        x_dev_data.append(text_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7c624163",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./project-data/dev.label.txt', 'r', encoding='utf-8') as f:\n",
    "    label_all = f.readlines()\n",
    "    for label in label_all:\n",
    "        if label[:-1] == 'rumour':\n",
    "            y_dev_data.append(1)\n",
    "        else:\n",
    "            y_dev_data.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b8544c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['covid 19 fact \\nare hand dryers effective in killing the new coronavirus \\n\\nshare the information with your loved ones and help keep them safe \\n\\nsource   who\\n           ']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re test\n",
    "strs = []\n",
    "str_e = \"COVID-19 Fact:\\nAre hand dryers effective in killing the newðŸ¤£ coronavirus?\\n\\nShare the information with your loved ones and help keep them safe.\\n\\nSource : WHO\\n#weatherbug #weather #knowbefore #wx #istayhomefor #wecan  #corona #quarantinelife #strongertogether #wewillprevail https://t.co/dsPnQLUpMy\"\n",
    "str_e = str_e.lower()\n",
    "str_e = remove_emoji(str_e)\n",
    "str_e = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', str_e, flags=re.MULTILINE)\n",
    "str_e = re.sub(r'@(\\w+)?', '', str_e, flags=re.MULTILINE)\n",
    "str_e = re.sub(r'#(\\w+)?','',str_e,flags=re.MULTILINE)\n",
    "str_e = re.sub(r'[^\\w\\s]',' ',str_e)\n",
    "strs.append(str_e)\n",
    "strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e9b708ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer txt to tsv\n",
    "import csv\n",
    "\n",
    "train_tsv_file = './project-data/train.tsv'\n",
    "train_tsv_columns = ['sentence','label']\n",
    "\n",
    "dev_tsv_file = './project-data/dev.tsv'\n",
    "dev_tsv_columns = ['sentence','label']\n",
    "\n",
    "\n",
    "train_data = zip(x_train_data, y_train_data)\n",
    "dev_data = zip(x_dev_data, y_dev_data)\n",
    "\n",
    "def ransfer_txt_to_tsv(output_tsv_file, output_tsv_columns, data):\n",
    "    with open(output_tsv_file, 'w', newline='') as f_output:\n",
    "        tsv_output = csv.writer(f_output)\n",
    "        tsv_output.writerow(output_tsv_columns)\n",
    "        for s, label in data:\n",
    "            tsv_output.writerow([s, label]) \n",
    "ransfer_txt_to_tsv(train_tsv_file, train_tsv_columns, train_data)\n",
    "ransfer_txt_to_tsv(dev_tsv_file, dev_tsv_columns, dev_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fa658b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert tokenizer\n",
    "from transformers import BertTokenizer\n",
    "bt = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e18ddbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f9a6afe2724c2b904170cc9ad37f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/558 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eabde54a830c469682abe0248f820b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/517M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3caad50ded5a497d92da432646e25bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/824k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e295c8d9df064e70b0c0eaac6bf63dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Please install emoji: pip3 install emoji\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    0,  4040,    90,   160,   255, 35006, 26940,  2612,    15,  1456,\n",
       "             7,   429,  6814,   499, 12952,    10,   156,     5,     3,    22,\n",
       "             2]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autoTokenizer\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
    "# line = \"SC has first two presumptive cases of coronavirus , DHEC confirms HTTPURL via @USER :cry:\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c4cea299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 15558,   733,    41,   803,  4691, 19110,  5433,    16,  1863,\n",
       "             6,   127,  1456,   991,     6,  1731,    30,    44,  1093,   784,\n",
       "            13,   272,   264,   106,  1309,  3182,    87,     2]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "input_ids = torch.tensor([tokenizer.encode(str_e)])\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "eb02edd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 15558, 733, 41, 803, 4691, 19110, 5433, 16, 1863, 6, 127, 1456, 991, 6, 1731, 30, 44, 1093, 784, 13, 272, 264, 106, 1309, 3182, 87, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_e_token = tokenizer(str_e)\n",
    "str_e_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "075664e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['co',\n",
       " '##vid',\n",
       " '19',\n",
       " 'fact',\n",
       " 'are',\n",
       " 'hand',\n",
       " 'dry',\n",
       " '##ers',\n",
       " 'effective',\n",
       " 'in',\n",
       " 'killing',\n",
       " 'the',\n",
       " 'new',\n",
       " 'corona',\n",
       " '##virus',\n",
       " 'share',\n",
       " 'the',\n",
       " 'information',\n",
       " 'with',\n",
       " 'your',\n",
       " 'loved',\n",
       " 'ones',\n",
       " 'and',\n",
       " 'help',\n",
       " 'keep',\n",
       " 'them',\n",
       " 'safe',\n",
       " 'source',\n",
       " 'who']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stop words\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# filtered_sentence = []\n",
    "word_tokens = bt.tokenize(str_e)\n",
    " \n",
    "# for w in word_tokens:\n",
    "#     if w not in stop_words:\n",
    "#         filtered_sentence.append(w)\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2754eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add [CLS] and [SEP]\n",
    "tokens = ['[CLS]'] + word_tokens + ['[SEP]'] \n",
    "# add [PAD]\n",
    "input_size = 512\n",
    "if len(tokens) < input_size:\n",
    "    #Padding token\n",
    "    tokens = tokens + ['[PAD]' for _ in range(input_size - len(tokens))] \n",
    "else:\n",
    "    # if tokens length > input_size, extract the first input_size-1 and add SEP\n",
    "    tokens = tokens[:input_size-1] + ['[SEP]'] \n",
    "\n",
    "# bert tokenizer word embedding\n",
    "import torch\n",
    "tokens_ids_tensor = torch.tensor(bt.convert_tokens_to_ids(tokens)) \n",
    "\n",
    "# attn_mask = (tokens_ids_tensor != 0).long()\n",
    "attn_mask = [1 if token != '[PAD]' else 0 for token in tokens]\n",
    "seg_ids = [0 for _ in range(len(tokens))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4eaaab8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['covid',\n",
       " 'fact',\n",
       " 'hand',\n",
       " 'dryers',\n",
       " 'effective',\n",
       " 'killing',\n",
       " 'new',\n",
       " 'coronavirus',\n",
       " 'share',\n",
       " 'information',\n",
       " 'loved',\n",
       " 'ones',\n",
       " 'help',\n",
       " 'keep',\n",
       " 'safe',\n",
       " 'source']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweet tokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "\n",
    "filtered_sentence_t = []\n",
    "tokens_t = tt.tokenize(str_e)\n",
    " \n",
    "for w in tokens_t:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence_t.append(w)\n",
    "filtered_sentence_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e29808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}