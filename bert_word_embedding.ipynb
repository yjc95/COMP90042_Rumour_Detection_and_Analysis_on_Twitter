{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f134bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "58de0496",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data, x_dev_data, x_test_data = [], [], []\n",
    "y_train_data, y_dev_data = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "34c93bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "21584375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the text group by line\n",
    "with open('./project-data/tweet-train-final.txt', 'r', encoding='utf-8') as f:\n",
    "    tweet_all = f.readlines()\n",
    "    for event in tweet_all:\n",
    "        tweets = json.loads(event)\n",
    "        text_event = ''\n",
    "        for k, v in tweets.items():\n",
    "            if 'data' in v:\n",
    "                text_event += v['data'][0]['text']\n",
    "                text_event = text_event.lower()\n",
    "        text_event = remove_emoji(text_event)\n",
    "        text_event = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text_event)\n",
    "        text_event = re.sub(r'@(\\w+)?', '', text_event)\n",
    "        text_event = re.sub(r'#(\\w+)?','',text_event)\n",
    "        text_event = re.sub(r'[^\\w\\s]', ' ', text_event)\n",
    "        \n",
    "        x_train_data.append(text_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "570ad71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'covid 19 fact \\nare hand dryers effective in killing the new coronavirus \\n\\nshare the information with your loved ones and help keep them safe \\n\\nsource   who\\n           '"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re test\n",
    "str_e = \"COVID-19 Fact:\\nAre hand dryers effective in killing the newðŸ¤£ coronavirus?\\n\\nShare the information with your loved ones and help keep them safe.\\n\\nSource : WHO\\n#weatherbug #weather #knowbefore #wx #istayhomefor #wecan  #corona #quarantinelife #strongertogether #wewillprevail https://t.co/dsPnQLUpMy\"\n",
    "str_e = str_e.lower()\n",
    "str_e = remove_emoji(str_e)\n",
    "str_e = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', str_e, flags=re.MULTILINE)\n",
    "str_e = re.sub(r'@(\\w+)?', '', str_e, flags=re.MULTILINE)\n",
    "str_e = re.sub(r'#(\\w+)?','',str_e,flags=re.MULTILINE)\n",
    "str_e = re.sub(r'[^\\w\\s]',' ',str_e)\n",
    "str_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fa658b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert tokenizer\n",
    "from transformers import BertTokenizer\n",
    "bt = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "89b04abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f9a6afe2724c2b904170cc9ad37f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/558 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eabde54a830c469682abe0248f820b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/517M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3caad50ded5a497d92da432646e25bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/824k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e295c8d9df064e70b0c0eaac6bf63dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Please install emoji: pip3 install emoji\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    0,  4040,    90,   160,   255, 35006, 26940,  2612,    15,  1456,\n",
       "             7,   429,  6814,   499, 12952,    10,   156,     5,     3,    22,\n",
       "             2]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autoTokenizer\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
    "# line = \"SC has first two presumptive cases of coronavirus , DHEC confirms HTTPURL via @USER :cry:\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "60e3693d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 15558,   733,    41,   803,  4691, 19110,  5433,    16,  1863,\n",
       "             6,   127,  1456,   991,     6,  1731,    30,    44,  1093,   784,\n",
       "            13,   272,   264,   106,  1309,  3182,    87,     2]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "input_ids = torch.tensor([tokenizer.encode(str_e)])\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9d232a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 15558, 733, 41, 803, 4691, 19110, 5433, 16, 1863, 6, 127, 1456, 991, 6, 1731, 30, 44, 1093, 784, 13, 272, 264, 106, 1309, 3182, 87, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_e_token = tokenizer(str_e)\n",
    "str_e_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "777bd61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_sentence = []\n",
    "word_tokens = bt.tokenize(str_e)\n",
    " \n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6dc3fd91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['co',\n",
       " '##vid',\n",
       " 'fact',\n",
       " 'hand',\n",
       " 'dry',\n",
       " '##ers',\n",
       " 'effective',\n",
       " 'killing',\n",
       " 'new',\n",
       " 'corona',\n",
       " '##virus',\n",
       " 'share',\n",
       " 'information',\n",
       " 'loved',\n",
       " 'ones',\n",
       " 'help',\n",
       " 'keep',\n",
       " 'safe',\n",
       " 'source']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d94ea1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'co',\n",
       " '##vid',\n",
       " 'fact',\n",
       " 'hand',\n",
       " 'dry',\n",
       " '##ers',\n",
       " 'effective',\n",
       " 'killing',\n",
       " 'new',\n",
       " 'corona',\n",
       " '##virus',\n",
       " 'share',\n",
       " 'information',\n",
       " 'loved',\n",
       " 'ones',\n",
       " 'help',\n",
       " 'keep',\n",
       " 'safe',\n",
       " 'source',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add [CLS] and [SEP]\n",
    "tokens = ['[CLS]'] + filtered_sentence + ['[SEP]'] \n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0445965e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['covid',\n",
       " 'fact',\n",
       " 'hand',\n",
       " 'dryers',\n",
       " 'effective',\n",
       " 'killing',\n",
       " 'new',\n",
       " 'coronavirus',\n",
       " 'share',\n",
       " 'information',\n",
       " 'loved',\n",
       " 'ones',\n",
       " 'help',\n",
       " 'keep',\n",
       " 'safe',\n",
       " 'source']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweet tokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "\n",
    "filtered_sentence_t = []\n",
    "tokens_t = tt.tokenize(str_e)\n",
    " \n",
    "for w in tokens_t:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence_t.append(w)\n",
    "filtered_sentence_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0cb72b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert tokenizer add [PAD]\n",
    "input_size = 512\n",
    "if len(tokens) < input_size:\n",
    "    #Padding token\n",
    "    tokens = tokens + ['[PAD]' for _ in range(input_size - len(tokens))] \n",
    "else:\n",
    "    # if tokens length > input_size, extract the first input_size-1 and add SEP\n",
    "    tokens = tokens[:input_size-1] + ['[SEP]'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3717e8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  2522, 17258,  2755,  2192,  4318,  2545,  4621,  4288,  2047,\n",
       "        21887, 23350,  3745,  2592,  3866,  3924,  2393,  2562,  3647,  3120,\n",
       "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,   102])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bert tokenizer word embedding\n",
    "import torch\n",
    "tokens_ids_tensor = torch.tensor(bt.convert_tokens_to_ids(tokens)) \n",
    "tokens_ids_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa541a64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}