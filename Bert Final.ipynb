{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from transformers import BertTokenizer,RobertaModel, RobertaTokenizerFast\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initial Train Dataset. Code refers workshop\n",
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, filename, maxlen):\n",
    "        self.df = pd.read_csv(filename).dropna().reset_index(drop=True)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        # self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Selecting the sentence and label at the specified index in the data frame\n",
    "        sentence = self.df.loc[index, 'sentence']\n",
    "        label = self.df.loc[index, 'label']\n",
    "\n",
    "        # Preprocessing the text to be suitable for BERT\n",
    "        tokens = self.tokenizer.tokenize(sentence)  # Tokenize the sentence\n",
    "        tokens = ['[CLS]'] + tokens + [\n",
    "            '[SEP]']  # Insering the CLS and SEP token in the beginning and end of the sentence\n",
    "        if len(tokens) < self.maxlen:\n",
    "            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))]  # Padding sentences\n",
    "        else:\n",
    "            tokens = tokens[:self.maxlen - 1] + ['[SEP]']  # Prunning the list to be of specified max length\n",
    "\n",
    "        tokens_ids = self.tokenizer.convert_tokens_to_ids(\n",
    "            tokens)  # Obtaining the indices of the tokens in the BERT Vocabulary\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids)  # Converting the list to a pytorch tensor\n",
    "\n",
    "        # Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
    "        attn_mask = (tokens_ids_tensor != 0).long()\n",
    "        return tokens_ids_tensor, attn_mask, label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create intsances of training and development dataset\n",
    "train_set = TrainDataset(filename='./project-data/train.tsv', maxlen=512)\n",
    "dev_set = TrainDataset(filename='./project-data/dev.tsv', maxlen=512)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creating intsances of training and development dataloaders\n",
    "# By the physical GPU attribute (6GB memory) of RTX 2060 on Dell G7 7590, the batch size is limited on 4\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=4, num_workers=0)\n",
    "dev_loader = DataLoader(dev_set, batch_size=4, num_workers=0)\n",
    "print(\"Done preprocessing training and development data.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initial Rumor Bert Classifer\n",
    "# Code refers workshop\n",
    "class RumorClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RumorClassifier, self).__init__()\n",
    "        # Instantiating BERT model object\n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        # self.bert_layer = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "        # Classification layer\n",
    "        # input dimension is 768 because [CLS] embedding has a dimension of 768\n",
    "        # output dimension is 1 because we're working with a binary classification problem\n",
    "        self.cls_layer = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, seq, attn_masks):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
    "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
    "        '''\n",
    "\n",
    "        # Feeding the input to BERT model to obtain contextualized representations\n",
    "        outputs = self.bert_layer(seq, attention_mask=attn_masks, return_dict=True)\n",
    "        cont_reps = outputs.last_hidden_state\n",
    "\n",
    "        # Obtaining the representation of [CLS] head (the first token)\n",
    "        cls_rep = cont_reps[:, 0]\n",
    "\n",
    "        # Feeding cls_rep to the classifier layer\n",
    "        logits = self.cls_layer(cls_rep)\n",
    "\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load GPU\n",
    "gpu = 0  # gpu ID\n",
    "print(\"Creating the covid classifier, initialised with pretrained bert-base-uncased parameters...\")\n",
    "# print(\"Creating the covid classifier, initialised with pretrained roberta-base parameters...\")\n",
    "net = RumorClassifier()\n",
    "net.cuda(gpu)  # Enable gpu support for the model\n",
    "print(\"Done creating the covid classifier.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Setting criterion and optimizer. Code refers workshop\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "opti = optim.Adam(net.parameters(), lr=2e-5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get accuracy from logits. Code refers workshop\n",
    "def get_accuracy_from_logits(logits, labels):\n",
    "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
    "    soft_probs = (probs > 0.5).long()\n",
    "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
    "    return acc\n",
    "\n",
    "# Get evaluation from logits. Code refers workshop\n",
    "def evaluate(net, criterion, dataloader, gpu):\n",
    "    net.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks, labels in dataloader:\n",
    "            seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n",
    "            logits = net(seq, attn_masks)\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
    "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
    "            count += 1\n",
    "\n",
    "    return mean_acc / count, mean_loss / count"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Main train function. Code refers workshop\n",
    "def train(net, criterion, opti, train_loader, dev_loader, max_eps, gpu):\n",
    "    best_acc = 0\n",
    "    st = time.time()\n",
    "    for ep in range(max_eps):\n",
    "\n",
    "        net.train()\n",
    "        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
    "            # Clear gradients\n",
    "            opti.zero_grad()\n",
    "            # Converting these to cuda tensors\n",
    "            seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n",
    "\n",
    "            # Obtaining the logits from the model\n",
    "            logits = net(seq, attn_masks)\n",
    "\n",
    "            # Computing loss\n",
    "            loss = criterion(logits.squeeze(-1), labels.float())\n",
    "\n",
    "            # Backpropagating the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimization step\n",
    "            opti.step()\n",
    "\n",
    "            if it % 100 == 0:\n",
    "                acc = get_accuracy_from_logits(logits, labels)\n",
    "                print(\"Iteration {} of epoch {} complete. Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep,\n",
    "                                                                                                             loss.item(),\n",
    "                                                                                                             acc, (\n",
    "                                                                                                                     time.time() - st)))\n",
    "                st = time.time()\n",
    "\n",
    "        dev_acc, dev_loss = evaluate(net, criterion, dev_loader, gpu)\n",
    "        print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\".format(ep, dev_acc, dev_loss))\n",
    "        if dev_acc > best_acc:\n",
    "            print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n",
    "            best_acc = dev_acc\n",
    "            torch.save(net.state_dict(), 'sstcls_{}.dat'.format(ep))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Setting number of epochs\n",
    "num_epoch = 16\n",
    "# fine-tune the model\n",
    "train(net, criterion, opti, train_loader, dev_loader, num_epoch, gpu)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Main testing function\n",
    "\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, filename, maxlen):\n",
    "        self.df = pd.read_csv(filename)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        # self.tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Selecting the sentence at the specified index in the data frame\n",
    "        sentence = self.df.loc[index, 'sentence']\n",
    "\n",
    "        # Preprocessing the text to be suitable for BERT\n",
    "        tokens = self.tokenizer.tokenize(sentence)  # Tokenize the sentence\n",
    "        tokens = ['[CLS]'] + tokens + [\n",
    "            '[SEP]']  # Insering the CLS and SEP token in the beginning and end of the sentence\n",
    "        if len(tokens) < self.maxlen:\n",
    "            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))]  # Padding sentences\n",
    "        else:\n",
    "            tokens = tokens[:self.maxlen - 1] + ['[SEP]']  # Prunning the list to be of specified max length\n",
    "\n",
    "        tokens_ids = self.tokenizer.convert_tokens_to_ids(\n",
    "            tokens)  # Obtaining the indices of the tokens in the BERT Vocabulary\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids)  # Converting the list to a pytorch tensor\n",
    "\n",
    "        # Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
    "        attn_mask = (tokens_ids_tensor != 0).long()\n",
    "\n",
    "        return tokens_ids_tensor, attn_mask\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loading test set\n",
    "test_set = TestDataset(filename='./project-data/test.tsv', maxlen=512)\n",
    "\n",
    "# Creating intsances of training and development dataloaders\n",
    "test_loader = DataLoader(test_set, batch_size=1, num_workers=0)\n",
    "\n",
    "print(\"Done preprocessing testing data.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Main prediction function\n",
    "def predict(net, test_loader, weight_file, result_file):\n",
    "    # load weight\n",
    "    net.load_state_dict(torch.load(weight_file))\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks in test_loader:\n",
    "            seq, attn_masks = seq.cuda(gpu), attn_masks.cuda(gpu)\n",
    "            logits = net(seq, attn_masks)\n",
    "            print(logits)\n",
    "            probs = torch.sigmoid(logits.unsqueeze(-1))\n",
    "            soft_probs = (probs > 0.5).long()\n",
    "            predictions.append(soft_probs.cpu().numpy().squeeze())\n",
    "\n",
    "    # Write into csv file\n",
    "    result = pd.DataFrame()\n",
    "    result['Predicted'] = predictions\n",
    "    result.to_csv(result_file,index=True,index_label='Id')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Best Bert weight file\n",
    "weight_file = 'sstcls_4.dat'\n",
    "\n",
    "# Result store into data directory\n",
    "result_file = './project-data./bert_result_8_4.csv'\n",
    "predict(net, test_loader, weight_file, result_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import covid dataset of task 2 as test_set to predict\n",
    "test_set = TestDataset(filename='./project-data/tweet_covid_bert.tsv', maxlen=512)\n",
    "\n",
    "# Creating intsances of training and development dataloaders\n",
    "test_loader = DataLoader(test_set, batch_size=1, num_workers=0)\n",
    "\n",
    "print(\"Done preprocessing testing data.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "weight_file = 'sstcls_4.dat'\n",
    "result_file = \"./project-data/tweet_covid_result_bert.csv\"\n",
    "\n",
    "predict(net, test_loader, weight_file, result_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}