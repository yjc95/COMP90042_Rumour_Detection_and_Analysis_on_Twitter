{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import torch.utils.data\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub('', text)\n",
    "\n",
    "\n",
    "def clean_text(text: str):\n",
    "    text = text.lower()\n",
    "    text = remove_emoji(text)\n",
    "    text = re.sub(r\"http\\S+\", '', text)\n",
    "    text = re.sub(r\"\\S*@\\S*\\s?\", '', text)\n",
    "    text = re.sub(r\"#\\S*\", \"\", text)\n",
    "    text = re.sub(r\"[^a-z^A-Z]\", ' ', text)  # remove anything except letters\n",
    "    text = re.sub(r\"\\s+\", ' ', text)\n",
    "\n",
    "    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text)\n",
    "    text = re.sub(r'@(\\w+)?', '', text)\n",
    "    text = re.sub(r'#(\\w+)?', '', text)\n",
    "\n",
    "    word_list = [w for w in text.split() if w not in stop_words]\n",
    "    text_clean = ''\n",
    "    for w in word_list:\n",
    "        text_clean += (w + ' ')\n",
    "    if text_clean != '':\n",
    "        return text_clean\n",
    "    return ''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "tweet_covid_text = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('./project-data/test.data.txt', 'r', encoding='utf-8') as f:\n",
    "    id_all = f.readlines()\n",
    "    for i in range(len(id_all)):\n",
    "        ids = id_all[i][:-1].split(',')\n",
    "        text_event = ''\n",
    "        for j in range(len(ids)):\n",
    "            file_path = './project-data/tweet-objects/' + ids[j] + '.json'\n",
    "            # try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f2:\n",
    "                tweet = json.load(f2)\n",
    "                text = tweet['text']\n",
    "                text = clean_text(text)\n",
    "                text_event += text\n",
    "            # except Exception:\n",
    "            #     pass\n",
    "        tweet_covid_text.append(text_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "test_covid_bert_file = './project-data/tweet_covid_bert.tsv'\n",
    "test_covid_bert_file_columns = ['sentence']\n",
    "test_data = tweet_covid_text\n",
    "\n",
    "def transfer_txt_to_tsv_test(output_tsv_file, output_tsv_columns, data):\n",
    "    with open(output_tsv_file, 'w', newline='') as f_output:\n",
    "        tsv_output = csv.writer(f_output)\n",
    "        tsv_output.writerow(output_tsv_columns)\n",
    "        for sentence in data:\n",
    "            if sentence:\n",
    "                tsv_output.writerow([sentence])\n",
    "            else:\n",
    "                tsv_output.writerow(['not a valid sentence'])\n",
    "\n",
    "if os.path.exists(test_covid_bert_file):\n",
    "    os.remove(test_covid_bert_file)\n",
    "\n",
    "transfer_txt_to_tsv_test(test_covid_bert_file, test_covid_bert_file_columns, test_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, filename, maxlen):\n",
    "        self.df = pd.read_csv(filename)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.maxlen = maxlen\n",
    "        # self.labels = df['label'].astype('category').tolist()\n",
    "        # self.texts = [tokenizer(text, padding='max_length',\n",
    "        #                         max_length=512,\n",
    "        #                         truncation=True,\n",
    "        #                         return_tensors='pt') for text in df['sentence']]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Selecting the sentence and label at the specified index in the data frame\n",
    "        sentence = self.df.loc[index, 'sentence']\n",
    "\n",
    "        # Preprocessing the text to be suitable for BERT\n",
    "        tokens = self.tokenizer.tokenize(sentence)  # Tokenize the sentence\n",
    "        tokens = ['[CLS]'] + tokens + [\n",
    "            '[SEP]']  # Insering the CLS and SEP token in the beginning and end of the sentence\n",
    "        if len(tokens) < self.maxlen:\n",
    "            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))]  # Padding sentences\n",
    "        else:\n",
    "            tokens = tokens[:self.maxlen - 1] + ['[SEP]']  # Prunning the list to be of specified max length\n",
    "\n",
    "        tokens_ids = self.tokenizer.convert_tokens_to_ids(\n",
    "            tokens)  # Obtaining the indices of the tokens in the BERT Vocabulary\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids)  # Converting the list to a pytorch tensor\n",
    "\n",
    "        # Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
    "        attn_mask = (tokens_ids_tensor != 0).long()\n",
    "\n",
    "        return tokens_ids_tensor, attn_mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "class CovidClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CovidClassifier, self).__init__()\n",
    "        #Instantiating BERT model object\n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        #Classification layer\n",
    "        #input dimension is 768 because [CLS] embedding has a dimension of 768\n",
    "        #output dimension is 1 because we're working with a binary classification problem\n",
    "        self.cls_layer = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, seq, attn_masks):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
    "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
    "        '''\n",
    "\n",
    "        #Feeding the input to BERT model to obtain contextualized representations\n",
    "        outputs = self.bert_layer(seq, attention_mask=attn_masks, return_dict=True)\n",
    "        cont_reps = outputs.last_hidden_state\n",
    "\n",
    "        #Obtaining the representation of [CLS] head (the first token)\n",
    "        cls_rep = cont_reps[:, 0]\n",
    "\n",
    "        #Feeding cls_rep to the classifier layer\n",
    "        logits = self.cls_layer(cls_rep)\n",
    "\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the sentiment classifier, initialised with pretrained BERT-BASE parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done creating the sentiment classifier.\n"
     ]
    }
   ],
   "source": [
    "gpu = 0  #gpu ID\n",
    "print(\"Creating the sentiment classifier, initialised with pretrained BERT-BASE parameters...\")\n",
    "net = CovidClassifier()\n",
    "net.cuda(gpu)  #Enable gpu support for the model\n",
    "print(\"Done creating the sentiment classifier.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done preprocessing testing data.\n"
     ]
    }
   ],
   "source": [
    "test_set = TestDataset(filename='./project-data/tweet_covid_bert.tsv', maxlen=512)\n",
    "\n",
    "# Creating intsances of training and development dataloaders\n",
    "test_loader = DataLoader(test_set, batch_size=1, num_workers=0)\n",
    "\n",
    "print(\"Done preprocessing testing data.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def predict(net, test_loader, weight_file, result_file):\n",
    "    # load weight\n",
    "    net.load_state_dict(torch.load(weight_file))\n",
    "\n",
    "    predictions = []\n",
    "    classes = [\"non-rumour\", \"rumour\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks in test_loader:\n",
    "            seq, attn_masks = seq.cuda(gpu), attn_masks.cuda(gpu)\n",
    "            logits = net(seq, attn_masks)\n",
    "            probs = torch.sigmoid(logits.unsqueeze(-1))\n",
    "            soft_probs = (probs > 0.5).long()\n",
    "            predictions.append(soft_probs.cpu().numpy().squeeze())\n",
    "\n",
    "# Write into csv file\n",
    "    result = pd.DataFrame()\n",
    "    result['Predicted'] = predictions\n",
    "    result.to_csv(result_file,index=True,index_label='Id')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "weight_file = 'sstcls_4.dat'\n",
    "result_file = \"tweet_covid_result_bert.csv\"\n",
    "prediction = predict(net, test_loader, weight_file, result_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}